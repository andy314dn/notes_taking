4.2 Linux kernel construction
from top-level kernel source tree, after successful build:
+ System.map: contains a human-readable list of kernel symbols
	and their respective addresses
+ vmlinux - the kernel proper: an architecture-specific ELF file in executable
	format. It is produced by the top-level kernel for every architecture.
	If the kernel was compiled with symbolic debug info, it will be 
	contained in the vmlinux image. This file is never booted directly
	It is a fully stand-alone, monolithic ELF image. Monolithic means
	vmlinux binary contains no unresolved external references.
Monolithic structure: entire kernel is compiled and linked as a single
	statically linked executable

output of kernel build system:
+ several common files (regardless of the architecture)
+ one or more architecture-specific binary modules

.lds: linker script file - a detailed recipe for how the kernel binary
	image should be linked

head.o (of vmlinux): in .../arch/arm/kernel/head.S
	This is an architecture-specific assembly lang source file that 
	performs very low-level kernel initialization
	It is the first code found in the binary image (vmlinux) created
	by the link stage

init_task.o: in .../arch/arm/kernel  or in .../init/init_task.c
	It sets up initial thread and task structures that the kernel requires.

built-in.o: a large collection of object modules.
	Each built-in.o object comes from a specific part of the kernel
	source tree. These are the binary objects that are included in
	the kernel image (including arch/arm/mm, arch/arm/common, 
	arch/arm/mach-ixp4xx, arch/arm/nwfpe, kernel/, mm/, fs/, ipc/,
	security/, crypto/, block/, arch/arm/lib, lib/, arch/arm/lib,
	lib/, drivers/, sound/, firmware/, net/, .tmp_kallsyms2).

The kernel contains some architecture-specific functionality, e.g. low-level
	context switching, hardware-level interrupt and timer processing,
	processor exception handling, and more. This is found 
	in .../arch/arm/kernel
Each architecture and machine type (processor/reference board) has different
	elements in the architecture-specific portions of the kernel


4.3 Kernel build sytem
dot-config file: the configuration blueprint for building a Linux kernel image

=m: dynamically loadable module, be inserted into the running kernel 
	after boot
=y: module is compiled and statically linked as part of the kernel image itself.
	For example, USB (=y) module would end up in the .../drivers/built-in.o
	composite binary

How the kernel config is accessed by various kernel modules:
	Most kernel software modules also read the configuration indirectly
	via the .config as follows. During the build process, the .config file
	is processed into a C header file found in the .../include/linux
	directory, called 'autoconf.h'. The kernel build files include this
	autoconf.h file into every kernel compile command line, using the
	-include gcc directive

make (no target): generates the kernel ELF file vmlinux and the default
	binary image for your chosen architecture (e.g. bzImage for x86).
	Also, it will build all the device driver modules (kernel loadable
	modules) specified by the configuration
	Many architectures and machine types require binary targets specific 
	to the architecture and bootloader in use. One of the more common
	architecture-specific targets is zImage. In many architectures, this
	is the default target image that can be loaded and run on the target
	embedded system.

4.4 Kernel configuration
Kconfig drives the configuration process for the features contained within 
	its subdirectory. The contents of Kconfig are parsed by the 
	configuration subsystem, which presents configuration choices to
	the user and contains help text associated with a given configuration
	parameter
	The configuration utility, e.g. gconf, reads the Kconfig files
	starting from the arch subdirectory's Kconfig file
	All Kconfig files taken together determine the configuration menu
	structure and configuration options presented to the user during
	kernel configuration


4.5 Kernel documentation:
The Linux Documentation Project: www.tlfp.org


5.1 Composite Kernel image: Piggy and Friends
Upon power-on, the bootloader in an ES is the first software to get processor
	control. After the bootloader has performed some low-level hardware
	initialization, control is passed to the Linux kernel
	This action can be a manual sequence of events to facilitate the
	development process (e.g. the user types interactive load/boot cmds
	at the bootloader prompt), or it can be an automated startup sequence
	typical of a production environment.

In the final sequence of steps in the kernel build process (listing 5-1), we
	see vmlinux, System.map, Image, then piggy.* are created. Among them 
	are the architecture-specifc head-xscale.o is created. Then vmlinux, 
	zImage are created. The architecture-specific object modules contain
	low-level utility routines needed to boot the kernel.
	+ Table 5-1 (page 138): description of low-level architecture objects
	  vmlinux, System.map, Image, head.o, piggy.gz, piggy.o, misc.o,
	  head-xscale.o, big-endian.o, vmlinux, zImage
	+ Figure 5-1 (page 139): Composite kernel image construction

The Image object
	The Image object is created from the vm-linux object. Image is the
	vmlinux ELF file stripped of redundant sections (notes and comments)
	and also stripped of any debugging symbols that might present.
	Image is the kernel proper (vmlinux) converted from ELF to binary form
	and stripped of debug symbols and the aforementioned .note* and 
	.comment sections.

Architecture objects
	They perform low-level architecture and processor-specific tasks.
	We should pay attention to the sequence of creating the object
	called 'piggy.o':
	+ First, the Image file (binary kernel image) is compressed using
	  gzip command to create piggy.gz (a compressed version of the binary
	  kernel Image)
	+ Next, an asm file called 'piggy.S' is assembled, which contains
	  a reference to the compressed 'piggy.gz'
	  ==> the binary kernel Image is being piggybacked as payload into
	      a low-level asm lang 'bootstrap loader'. This bootstrap loader
	      initializes the processor and required memory regions,
	      decompresses the binary kernel Image, and loads it into the
	      proper place in system memory before passing control to it.
	      (piggy.S is in .../arch/arm/boot/compressed/piggy.S)
	+ The net result of piggy.S: to contain the compressed binary kernel
	  Image as a payload within another image - the bootstrap loader

Bootstrap loader
	Many architectures use a 'bootstrap loader' (second-stage loader) to
	load the Linux kernel image into memory.
	It performs checksum verification of the kernel image, decompress and
	relocate the kernel image.
	+ bootloader: controls the board upon power-up and does not rely on the
	  Linux kernel in any way
	+ bootstrap loader: acts as the 'glue' between a bare metal bootloader
	  and the Linux kernel. It provides a proper context for the kernel
	  to run in, as well as performs the necessary steps to decompress
	  and relocate the kernel binary image
	The bootstrap loader is concatenated to the kernel image for loading.
	Functions of the bootstrap loader:
	+ low-level assembly language processor initialization
	+ decompression and relocation code
	+ other processor-specific initialization


5.2 Initialization Flow of Control (from bootloader to kernel)
bootloader: the low-level component that resides in system nonvolatile memory
	(Flash or ROM). It takes control immediately after the power has been
	applied. It is typically a small, simple set of routines designed
	primarily to do low-level initialization, OS image loading, and
	system diagnostics... Finally, it contains logic for loading & passing
	control to another program, usually OS such as Linux.
	When power is first applied, the bootloader is invoked and proceeds to
	load the OS. When the bootloader locates and loads the OS image (which
	could be resident locally in Flash, on a hard drive, or via a LAN or 
	other device), control is passed to that image.
	On our particular XScale platform, the bootloader passes control to
	our head.o module at the label 'start' in the bootstrap loader.
	The bootstrap loader prepended to the kernel image has a single primary
	responsibility: to create the proper environment to decompress and 
	relocate the kernel and pass control to it.
	Control is passed from the bootstrap loader directly to the kernel
	proper, to a module called head.o.
	When the bootstrap loader has completed its job, control is passed to
	the kernel proper's head.o, and from there to start_kernel() in main.c.

Kernel Entry point: head.o (in .../arch/<ARCH>/kernel/head.S
	The intention of the kernel developers was to keep the architecture-
	specific 'head.o' module very generic, without any specific machine
	dependencies.
	head.o module performs architecture- and often CPU-specific 
	initialization in preparation for the main body of the kernel. It does:
	+ check for valid processor and architecture
	+ create initial page table entries
	+ enable the processor's memory management unit (MMU)
	+ establish limited error detection and reporting
	+ jump to the start of the kernel proper, start_kernel() in main.c

	When the control is first passed to the kernel's head.o from the
	bootstrap loader, the processor is operating in 'real mode'. It means
	the logical address contained in the processor's Program Counter is 
	the actual physical address driven onto the processor's electrical
	memory address pins. Soon after the processor's registers and kernel
	data structures are initialized to enable memory translation, the
	processor's MMU is turned on. Suddenly, the address space as seen by 
	the processor is yanked (giat lai) from beneath it and replaced by an
	arbitrary virtual addressing scheme determined by the kernel developers
	==> physical addresses are replaced by logical addresses the moment
	the MMU is enabled.

	There is limited available mapping at this early stage of the kernel
	boot process. Therefore, at this early stage in the boot cycle, you
	are pretty much guaranteed NOT to have any error messages to help you
	figure out what's wrong.

Kernel startup: main.c
	The final task performed by the kernel's own head.o module is to pass
	the control to the primary kernel startup file written in C. Control
	is passed from the kernel's first object module (head.o) to the C
	lang routine start_kernel() located in .../init/main.c. Here the kernel
	begins to develop a life of its own.
	main.c does the bulk of the post-assembly-language startup work for the
	Linux kernel, from initializing the first kernel thread all the way to
	mounting a root FS and executing the very first user space Linux app
	program.
	The function start_kernel() is by far the largest function in main.c.
	Most of the Linux kernel initialization takes place in this routine.

Architecture setup: setup_arch() in .../arch/arm/kernel/setup.c
	setup_arch() takes a single parameter - a pointer to the kernel cmd
	line: e.g. setup_arch(&command_line);. This statement calls an arch-
	specific setup routine responsible for performing initialization tasks
	common across each major architecture. Among other functions, 
	setup_arch() calls functions that identify the specific CPU and
	provides a mechanism for calling high-level CPU-specific initialization
	routines. One such function, called directly by setup_arch(), is
	setup_processor(), found in .../arch/arm/kernel/setup.c. This function
	verifies the CPU ID and revision, calls CPU-specific initialization
	functions, and displays several lines of info on the console during
	boot.
	One of the final actions of the architecture setup routines is to 
	perform any machine-dependent initialization. For ARM, you will find
	machine-specific initialization in the .../arch/arm/mach-* series of
	directories.

5.3 Kernel command-line processing
	Following the arch setup, main.c performs generic early kernel 
	initialization and then displays the kernel command line, e.g.
	   Kernel command line: console=ttyS0,115200 root=/dev/nfs ip=dhcp
	In this example, the kernel being booted is instructed to open a 
	console device on serial port device ttyS0 at a baud rate of 115Kbps.
	It is being instructed to obtain its initial IP adr info from a DHCP
	server and to mount a root FS via the NFS protocol.

	Linux typically is launched by a bootloader (or bootstrap loader) with
	a series of parameters that have come to be called the 'kernel command
	line'. Although you don't actually invoke the kernel using a command
	prompt from a shell, many bootloaders can pass parameters to the kernel
	in a fashion that resembles this well-known model.
	The bootstrap loader builds the kernel cmd line from a configuration
	file and passes it to the kernel during the boot process. These cmd
	line parameters are a boot mechanism to set the initial configuration
	necessary for proper boot on a given machine.

	command-line parameters are contained in kernel-parameters.txt (in Doc)

	embedded engineers need to comprehend the complexities of a linker
	script file to fully understand the mechanism of adding kernel cmd-line
	parameters for their ow specific needs. (read GNU LD manual at
	https://sourceware.org/binutils/docs/ld/index.html)

the __setup Macro
	Consider the specification of the console device. We want a console 
	initialized early in the boot cycle so that we have a destination for
	message during boot. This initialization takes place in a kernel obj
	called 'printk.o' (.../kernel/printk.c). The console initialization
	routine is called console_setup() and takes the kernel cmd-line param
	string as its only argument.
	The challenge is to communicate the console parameters specified on the
	kernel cmd line to the setup and device driver routines that require
	this data in a modular and general fashion.
	==> What is needed is a flexible and generic way to pass these kernel
	cmd-line parameters to their consumers.

	A special macro defined in .../include/linux/init.h is used to
	associate a portion of the kernel cmd-line string with a function
	that will act on that portion of the string.
	In .../kernel/printk.c, we only pay attention to the invocation of the
	__setup macro. It expects two args: a string literal and a function
	pointer, e.g. __setup("console=", console_setup);

	The __setup macro can be thought as a registration function for the
	kernel command-line console parameter. In effect, it says: when the
	'console=' string is encountered on the kernel cmd line, invoke the 
	function represented by the second __setup macro argument (e.g. the
	console_setup() function). But how is this info communicated to the 
	early setup code, outside this module, which has no knowledge of the
	console function? The mechanism relies on lists built by the linker.

	The details are hidden in a set of macro whose objective is to build 
	a static list of string literals associated with function pointers.
	This list is emitted by the compiler in a separately named ELF section
	in the final 'vmlinux' ELF image.

	See the definition of __setup family of macros in 
	.../include/linux/init.h. It contains some macros:
	__used macro: tell the compiler to emit the function or variable, even
	if the optimizer determines that it is 'unused'.
	__attribute__ ((aligned)) macro: tell the compiler to align the 
	structures on a specific boundary (e.g. sizeof(long))
	+ The compiler generates an array of characters (a string pointer) 
	  called __setup_str_console_setup[] initialized to contain the 
	  string 'console='.
	+ Next, the compiler generates a structure that contains three members:
	  a pointer to the kernel cmd-line string (the array just declared),
	  a pointer to the setup function itself, and a simple flag.
	+ The key to the magic here is the section attribute attached to
	  the structure. This attribute instructs the compiler to emit this
	  structure into a special section within the ELF object module,
	  called '.init.setup'. 
	+ During the link stage, all the structures defined using the __setup
	  macro are collected and placed in this .init.setup section, in effect
	  creating an array of these structures.
	+ In .../init/main.c, this data is accessed and used (Listing 5-6)
	  + kernel command line is parsed in .../kernel/params.c
	  + the two external structure pointers __setup_start, __setup_end are
	    defined in a linker script file, not in a C source or header file.
	    These labels mark the start and end of the array of 
	    'obs_kernel_param' structures that were placed in the .init.setup
	    section of the object file.
	  + The code in Listing 5-6 scans all these structures via the pointer
	    p to find a match for this particular kernel command-line param.
	    In this case, the code is searching for the string 'console=' and
	    finds a match. From the relevant structure, the function pointer
	    element returns a pointer the console_setup() function, which is
	    called with the balance of the parameter (the string 
	    'ttyS0,115200') as its only argument. This process is repeated for
	    every element in the kernel command line until the kernel cmd-line
	    has been exhausted.
	The technique just described, collecting objects into lists in uniquely
	named ELF sections, is used in many places in the kernel.
	Another example of this technique is the use of the __init family of 
	macros to place one-time initialization routines into a common section
	in the object file. Their cousin __initconst, used to mark one-time-use
	data items, is used by the __setup macro. Functions and data marked as
	initialization using these macros are collected into specifically 
	named ELF sections. Later, after these one-time initialization 
	functions and data objects have been used, the kernel frees the memory
	occupied by these items, e.g. kernel message: Freeing init memory:29K

	The use of symbol names preceded with 'obsolete_' is because the 
	kernel developers are replacing the kernel cmd-line processing 
	mechanism with a more generic mechanism for registering both boot time
	and loadable module parameters. New development is expected to use the
	family of functions defined by the kernel header file
	.../include/linux/moduleparam.h - most notably, the family of
	module_param* macros.
	The new mechanism maintains backward compatibility by including an 
	unknown function pointer arg in the parsing routine. Thus, parameters
	that are unknown to the 'module_param*' infrastructure are consider
	unknown, and the processing falls back to the old mechanism under
	control of the developer. See .../kernel/params.c and parse_args()
	calls in .../init/main.c

	The purpose of the 'flag member' of the 'obs_kernel_param' structure
	created by the __setup macro: the flag in the structure, called early,
	is used to indicate whether this particular command-line parameter
	was already consumed earlier in the boot process. Some command-line
	parameters are intended for consumption very early in the boot process,
	and this flag provides a mechanism for an early parsing alg. You will
	find a function in main.c called do_early_param() that traverses the
	linker-generated array of __setup-generated structures and processes,
	each one marked for early consumption.

5.4 Subsystem Initialization
Many kernel subsystems are initialized by the code found in main.c. The linker
builds lists of function pointers to various initialization routines, and a
simple loop is used to execute each in turn.

The *__initcall macros (Listing 5-7, page158)
	__init macro: applies a 'section' attribute to declare that this fnc
	gets placed in a section called '.init.text' in the 'vmlinux' ELF file.
	The purpose of placing this function in a special section of the obj
	file is so that the memory space it occupies can be reclaimed when
	it is no longer needed.

	the marcro 'arch_initcall(customize_machine)' - part of a family of 
	macros defined in .../include/linux/init.h. These macros declare a 
	data item based on the function's name. They also use the 'section'
	attribute to place this data item in a uniquely named section of the
	'vmlinux' ELF file. The benefit of this approach is that main.c can
	call an arbitrary initialization function for a subsystem that it has
	no knowledge of. The name of the section is '.initcallN.init', where
	N is the level defined, between 1 and 7. There is a section named for
	each of the seven levels with an 's' appended. This is intended to be
	a 'synchronous' initcall. The data item is assigned the address of the
	function being named in the macro. The data item is placed in the 
	kernel's obj file in a section called .initcall3.init.
	The level (N) is used to provide an ordering of initialization calls.

	The family of *_initcall macros can be thought as registration 
	functions for kernel subsystem initialization routines that need to
	be run once at kernel startup and never used again. These macros
	provide a mechanism for causing the initialization routine to be 
	executed during system startup and a mechanism to discard the code
	and reclaim the memory after the routine has been executed.

5.5 The 'init' Thread
The code found in .../init/main.c is responsible for bringing the kernel to
life. After start_kernel() performs some basic kernel initialization, calling
early initialization functions explicitly by name, the very first kernel thread
is spawned.  This thread eventually becomes the kernel thread called 'init()',
with a process ID (PID) of 1. init() becomes the parent of all Linux processes
in user space.  At this point in the boot sequence, two distinct threads are
running: that represented by start_kernel(), and now init(). The former goes on
to become the 'idle' process, having completed its work. The latter becomes
the 'init' process.

The start_kernel() function calls rest_init(). The kernel's 'init' process is
spawned by the call to kernel_thread(), with the function 'kernel_init' as its
first parameter. 'init' goes on to complete the rest of the system 
initialization, while the thread of execution started by start_kernel() loops
forever in the call to cpu_idle().
The reason for this structure is: We see that start_kernel(), a relatively
large function, was marked with the __init macro. This means that the memory
it occupies will be reclaimed during the final stages of kernel initialization.
It is necessary to exit this function and the address space it occupies before
reclaiming its memory. The answer to this is for start_kernel() to call
rest_init(), a much smaller piece of memory that becomes the idle process.

Initialization via 'initcalls'
When kernel_init() is spawned, it eventually calls 'do_initcalls()', which is
the function responsible for calling most of the initialization functions
registered wit the *_initcall family of macros.
Note for two loop boundaries in do_initcalls() and do_pre_smp_initcalls():
__initcall_start and __initcall_end. These labels are not found in any C source
or header file. They are defined in the linker script file used during the link
stage of vmlinux. These labels mark the beginning and end of the list of
initialization functions populated using the *_initcall family of macros. You
can see each of the labels by looking at the 'System.map' file in the top-lv
kernel directory.

Final boot steps
Having spawned the 'kernel_init()' thread, and after all the various
initialization calls have completed, the kernel performs its final steps in the
boot sequence. These include free the memory used by the initialization 
functions and data, opening a system console device, and starting the first
user space process.
If the code proceeds to the end of the init_post() function, a kernel panic
results. One way or another, one of the 4 run_init_process() commands must 
proceed without error.  The 'run_init_process()' function does not return on
successful invocation. It overwrites the calling process with the new one,
effectively replacing the current process with the new one. It uses the 
familiar execve() system call for this functionality. The most common system
configuration spawn /sbin/init as the user-land initialization process.

One option available to the embedded system developer is to use a custom user-
land initialization program. That is the purpose of the conditional statement
in the code snippet of init_post(). If execute_command is non-null, it points
to a string containing a custom user-supplied command to be executed in user
space. The developer specifies this command on the kernel command line, and it
is set via the __setup macro we examined earlier in this chapter.
A sample kernel cmd line might look like this:
==>initcall_debug init=/sbin/myinit console=ttyS1,115200 root=/dev/hda1
   This kernel cmd line instructs the kernel to display all the initialization
   routines as they are invoked (initcall_debug), configures the initial
   console device as /dev/ttyS1 at 115Kbps, and executes a custom user space
   initialization process called 'myinit', which is located in the /sbin dir
   on the root FS. It directs the kernel to mount its root FS from the device
   /dev/hda1, which is the first IDE hard drive.
   NOTE: in general, the order of parameters given on the kernel command line
   is irrelevant.


6.1 Root File System
Linux requires a Root File System to realize the benefits of its services.
The root FS refers to the file system mounted at the base of the file system
hierarchy, designated simply as /. The root FS is simply the first file system
mounted at the top of the file system hierarchy.
The root FS has special requirements for a Linux system. Linux expects the 
root FS to contain programs and utilities to boot the system, initialize
services such as networking and a system console, load device drivers, and
mount additional file systems.

FHS - File System Hierarchy Standard
The FHS establishes a minimum baseline of compatibility between Linux
distribution and application programs. The FHS standard allows the application
software (and developers) to predict where certain system elements, including
files and directories, can be found in the FS.

File System Layout
Where space is a concern, many ES developers create a very small root FS on a 
bootable device (e.g. Flash memory). Later, a larger FS is mounted from another
device, perhaps a hard disk or NFS server. It is not uncommon to mount a larger
root FS on top of the original small one.
A simple Linux root FS might contain the following top-level directory entries:
	bin, dev, etc, home, lib, sbin, usr, var, tmp

Minimal File System
+ bin (busybox, sh->busybox)
+ dev (console)
+ etc( + init.d (rcS))
+ lib (ld-2.3.2.so, ld-linux.so-2->ld-2.3.2.so, libc-2.3.2.so, ...)
This tiny root FS boots and provides the user with a fully functional command
prompt on the serial console. Any commands that have been enabled in busybox
are available to the user.
The file in /dev is a device node required to open a console device for IO.
the rcS file in the /etc/init.d directory is the default initialization script
processed by 'busybox' on startup. Including rcS silences the warning message
issued by 'busybox' whenever rcS is missing.
There are also two libraries, glibc and the Linux dynamic loader.  glibc
contains the standard C library functions, such as printf() and many others
that most app programs depend on. The Linux dynamic loader is responsible for
loading the binary executable into memory and perform the dynamic linking
required by the app's reference to shared library functions. The two additonal
soft links provide version immunity and backward comptibility for the libraries
themselves and are found on all Linux systems.

6.2 Kernel's last boot steps
The final sequence of events for the kernel thread called kernel_init spawned
by the kernel during the final stages of boots. The run_init_process() is a
small wrapper around the execve() function, which is a kernel system call
with rather interesting behavior. The execve() function never returns if no
error conditions are encountered in the call. The memory space in which the 
calling thread executes is overwritten by the called program's memory image. In
effect, the called program directly replaces the calling thread, including 
inheriting its Process ID (PID)
Essentially, this is the start of user space processing.
A key ingredient of the init processes: They are all programs that are expected
to reside on a root FS. Therefore, we know that we must at least satisfy the
kernel's requirement for an 'init' process that can execute within its own
environment.

First User Space Program
On most Linux systems, /sbin/init is spawned by the kernel on boot. This is why
it is attempted first. Effectively, this becomes the first user space program
to run. To review, this is the sequence:
	1. Mount the root file system
	2. Spawn the first user space program, which becomes /sbin/init
It has soft link to busybox ==> It causes 'busybox' to be executed by the
kernel as the initial process while also satisfying the common requirement for
a shell executable from user space.

Resolving Dependencies
For every process you place on your root FS, you must also satisfy its 
dependencies. Most processes have 2 categories of dependencies:
	+ those that are needed to satisfy unresolved references within a
	  dynamically linked executable
	+ external configuration or data files that an application might need
For example:
the init process is a dynamically linked executable. To run init, we need to 
satisfy its library dependencies using the tool 'ldd'
to satisfy the second category of dependencies for an executable, the 
configuration and data files that it might need, there is little substitute
for some knowledge about how the subsystem works. That is, init expects to read
its operational configuration from a data file called 'inittab' located on /etc

6.3 The init Process
The 'init' program together with a family of startup scripts implements what is
commonly called System V Init, from the original UNIX System V that used this
schema. We will know examine this powerful system configuration and control
utility.

init is the ultimate parent of all user space processes in Linux system.
Furthermore, init provides the default set of environment parameters for all
other processes to inherit, such as the initial system PATH.
Its primary role is to spawn additional processes under the direction of a 
special configuration file. This configuration file is usually stored as 
/etc/inittab. 
init has the concept of a runlevel. A runlevel can be thought of
as 'a system state'. Each runlevel is defined by the services that are enabled
and programs that are spawned upon entry to that runlevel.

init can exist in a single runlevel at any given time. Runlevels used by init
include runlevels from 0 to 6 and a special runlevel called S.
For each runlevel, a set of startup and shutdown scripts is usually provided
that define the action a system should take for each runlevel. Actions to 
perform for a given runlevel are determined by the /etc/inittab configuration
file.

The runlevel scripts are commonly found under a directory called 
/etc/rc.d/init.d. Here you will find most of the scripts that enable and 
disable individual services. Services can be configured manually by invoking
the script and passing one of the appropriate args to the scripts, such as
start, stop, or restart.

A runlevel is defined by the services that are enabled at that runlevel. Most
Linux distros contain a directory structure under /etc that contains symbolic
links to the service scripts in /etc/rc.d/init.d. These runlevel directories
typically are rooted at /etc/rc.d. Under this directory, you will find a series
of runlevel directories that contain startup and shutdown specifications for
each runlevel. init simply executes these scripts upon entry and exit from a
runlevel. The scripts define the system state, and inittab instructs init which
scripts to associate with a given runlevel.

Each of the runlevels is defined by the scripts contained in rcN.d, where N is
the runlevel. Inside each rcN.d directory, you will find numerous symlinks
arranged in a specific order. These symlinks start with a K(ill) or S(tart).
The top level script that executes these service startup and shutdown scripts
is defined in the 'init' configuration file.

inittab
When init is started, it reads the system configuration file /etc/inittab. This
file contains directives for each runlevel, as well as directives that apply to
all runlevels.
Each runlevel is associated with a script, which must be created by the 
developer for the desired actions in each runlevel. When this file is read by
init, the first script to be executed is /etc/rc.sysinit. This is denoted by
the 'sysinit' tag (e.g. si::sysinit:/etc/rc.sysinit). Then init enters runlevel
2 and executes the script defined for runlevel 2. From this example, this would
be /etc/init.d/runlvl2.startup (e.g. 12:2:wait:/etc/init.d/runlvl2.startup). 
Because of the 'wait' tag, init waits until the script completes before
continuing. When the runlevel 2 script completes, init spawns a shell on the
console (through the /bin/sh symbolic link) (e.g. con:2:respawn:/bin/sh). The
respawn keyword instructs init to restart the shell each time it detects that
it has exited.

6.4 Initial RAM Disk
The Linux kernel contains two mechanisms to mount an early root FS to perform
certain startup-related system initialization and configuration.
+ legacy method: the initial ramdisk or initrd
+ newer method:  initramfs

The legacy method for enabling early user space processing is known as the 
initial RAM disk, or simply 'initrd'.  Support for this functionality must be
compiled into the kernel. 

The initial RAM disk is a small, self-contained root FS that usually contains
directives to load specific device drivers before the completion of the boot
cycle. An 'initrd' is frequently used to load a device driver that is required
in order to access the real boot FS.

booting with 'initrd'
To use the initrd functionality, the bootloader gets involved on most archs to
pass the initrd image to the kernel.  A common scenario is that the bootloader
loads a compressed kernel image into memory and the loads an 'initrd' image
into another section of available memory. In doing so, it becomes the
bootloader's responsibility to pass the load address of the 'initrd' image to
the kernel before passing control to it. The kernel must know where the initrd
image is located so it can load it.
Some architectures and platforms construct a single composite binary image.
This scheme is used when the bootloader does not have specific Linux support
for loading 'initrd' images. In this case, the kernel and 'initrd' image are
simply concatenated into a single composite image. You will find reference to
this type of composite image in the kernel makefiles as 'bootpImage'.
So how does the kernel know where to find the 'initrd' image? We simply pass
the 'initrd' image start address and size to the kernel via the kernel command
line, e.g.
	console=ttyS0,115200 root=/dev/nfs			\
	  nfsroot=192.168.1.9:/home/chris/sandbox/omap-targe	\
	  initrd=0x10800000,0x14af47
This kernel command line defines the following kernel behavior:
	+ specify a single console on device ttyS0 at 115 kilobaud
	+ mount a root FS via NFS, the network FS
	+ find the NFS root FS on host 192.168.1.9 (from the directory
	  /home/chris/sandbox/omap-target)
	+ load and mount an initial ramdisk from physical memory location
	  0x10800000, which has a size of 0x14AF47 bytes (compressed)

bootloader support for initrd
The U-Boot bootloader was designed with support for directly booting the Linux
kernel. Using U-Boot, it is easy to include an 'initrd' image with the kernel
image. Here is the typical boot sequence containing an initial ramdisk image:
	tftp 0x10000000 kernel-uImage
	tftp 0x10800000 initrd-uboot
	bootm 0x10000000 0x10800040
The 'tftp' command causes U-Boot to download the kernel image from a TFTP 
server. The kernel image is downloaded and placed into the base of this target
system's memory at the 256MB (0x10000000). Then a second image, the initial
ramdisk image, is downloaded from a TFTP server into memory at a higher mem
address (256MB + 8MB in this example). Finally, we issue the U-Boot 'bootm'
command, which is the 'boot from memory' command. The bootm command takes 2
arguments: the address of the Linux kernel image, optionally followed by an 
address representing the location of the initial ramdisk image.
Special note of one feature of the U-Boot bootloader: it fully supports loading
kernel and ramdisk images over an Ethernet connection. You can get a kernel &
ramdisk image onto your board in other ways as well. You can flash them into 
your Flash memory using a HW-based Flash programming tool, or you can use a 
serial port and download the kernel and FS images via RS-232.

'initrd' magic: 'linuxrc'
When the kernel boots, first it detects the presence of the 'initrd' image. 
Then it copies the compressed binary file from the specified physical location
in RAM into a proper kernel ramdisk and mounts it as the root FS. The magic of
'initrd' comes from the contents of a special file within the 'initrd' image.
When the kernel mounts the intial ramdisk, it looks for a specific file called
'linuxrc'. It treats this file as a script file and proceeds to execute the
commands contained therein. This mechanism enables the system designer to 
specify the behavior of 'initrd'. Here is a sample 'linuxrc' file:

#!/bin/sh

echo 'Greetings: this is 'linuxrc' from Initial Ramdisk'
echo 'Mounting /proc filesystem'
mount -t proc /proc /proc

busybox sh

In practice, this file would contain directives required before we mount the
real root FS. One example might be to load CompactFlash drivers to obtain a 
real root FS from a CompactFlash device. For the purposes of this example, we
simply spawn a 'busybox' shell and halt the boot process for examination. If
you were to type the 'exit' command here, the kernel would continue its boot
process until complete.
After the kernel copies the ramdisk from physical memory into a kernel ramdisk,
it returns this physical memory to the available memory pool. You can think of
this as transferring the 'initrd' image from physical memory at the hard-coded
address into the kernel's own virtual memory (in the form of a kernel ramdisk
device).

the 'initrd' plumbing
As part of the Linux boot process, the kernel must locate and mount a root FS.
Late in the boot process, the kernel decides what and where to mount in a 
function called prepare_namespace(), which is found in .../init/do_mounts.c. If
'initrd' support is enabled in the kernel and the kernel command line is so
configured, the kernel decompresses the compressed 'initrd' image from physical
memory and eventually copies the contents of this file into a ramdisk device
(/dev/ram). At this point, we have a proper file system on a kernel ramdisk.
After the file system has been read into the ramdisk, the kernel effectively
mounts this ramdisk device as its root file system. Finally, the kernel spawns
a kernel thread to execute the 'linuxrc' file on the 'initrd' image.

When the 'linuxrc' script has completed execution, the kernel unmounts the 
initrd and proceeds with the final stages of system boot. If the real root dev
has a directory called /initrd, Linux mounts the 'initrd' file system on this
path (a mount point). If this directory does not exist in the final root FS, 
the 'initrd' image is simply discarded.

If the kernel command line contains a 'root=' parameter specifying a ramdisk
(e.g. root=/dev/ram0), the previously described 'initrd' behavior changes in
2 important ways.
	+ First, the processing of the 'linuxrc' executable is skipped.
	+ Second, no attempt is made to mount another file system as root. This
	  means that you can have a Linux system with initrd as the only rootFS

6.5 Using 'initramfs'
'initramfs' is the preferred mechanism for executing early user space programs.
It is enabled using the same configuration selections as shown in Fig 6-1. Its
purpose is also similar: to enable loading of drivers that might be required
before mounting the read (final) root file system.

The technical implementation details differ significantly between 'initrd' and
'initramfs', e.g. initramfs is loaded before the call to do_basic_setup() (
do_basic_setup() is called from .../init/main.c and calls do_initcalls(). This
causes driver module initialization routines to be called), which provides a 
mechanism for loading firmware for devices before its driver has been loaded.

initramfs is much easier to use. initramfs is a cpio archive, whereas initrd is
a gzipped file system image. This simple difference contributes to the ease of
use of initramfs and removes the requirement that you must be root to create it.
It is integrated into the Linux kernel source tree, and a small default (nearly
empty) image is built automatically when you build the kernel image. Make
changes to it is far easier than building and loading a new initrd image.

.../usr directory is where the 'initramfs' image is built
A build script in .../scripts called gen_initramfs_list.sh defines a default
list of files that will be included in the 'initramfs' archive, e.g.
	dir	/dev 0755 0 0
	nod	/dev/console 0600 0 0 c 5 1
	dir 	/root 0700 0 0
This produces a small default directory structure containing the /root and /dev
top-level directories, as well as a single device node representing the console
The details of HOW-TO specify items for 'initramfs' FS are described in the
kernel doc at .../Documentation/filesystem/ramfs-rootfs-initramfs.txt
The first line produces a directory entry (dir) called /dev, with 0755 file
permissions and a user-id and group-id of 0 (root).
The second line defines a device node (nod) called /dev/console, with file
permissions of 0600, user and group IDs of 0 (root), being a character dev (c),
with major number 5 and minor number 1.
The third line creates another directory called /root similar to the /dev 
specifier.

customizing 'initramfs'
There are 2 ways to customize the 'initramfs': 
	+ either create a cpio archive w/ your required files
	+ specify a list of directories and files whose contents are merged 
	  with the default created by gen_initramfs_list.sh
You specify a source for your 'initramfs' files via the kernel-configuration
facility. Enable INITRAMFS_SOURCE in your kernel configuration, and point it to
a location on your development workstation. The kernel build system will use
those files as the source for your 'initramfs' image.

First, we will build a file collection containing the files we want for a 
minimal system. Because 'initramfs' is supposed to be small and lean, we'll 
build it around a statically compiled 'busybox'. Compiling busybox statically
means it is not dependent on any system libraries. We need very little beyond
busybox: a device node for the console in directory called /dev and a symlink
pointing back to busybox called init. Finally, we'll include a busybox startup
script to spawn a shell for us to interact with after booting into this 
initramfs (suppose they are in /usr/myinitramfs_root/)

When we point the kernel configuration parameter INITRAMFS_SOURCE to the dir
where this file structure lives, it automatically builds the 'initramfs'
compressed cpio archive and links it into the kernel image.

The reason for the 'init' symlink should be noted. When the kernel is 
configured for 'initramfs', it searches for an executable file called /init on
the root of the 'initramfs' image. If it finds it, it executes it as the 'init'
process with PID (process ID) set to 1. If it does not find it, it skips
'initramfs' and proceeds with normal root FS processing. This logic is found in
.../init/main.c. A character pointer called ramdisk_executable_command contains
a pointer to this initialization command. By default it is set to the string
"/init"

A kernel command-line parameter called 'rdinit=', when set, overrides this init
specifier much the same way that 'init=' does. To use it, simply add it to
your kernel cmd line. For example, we could have set 'rdinit=/bin/sh' on our
kernel cmd-line to directly call the busybox shell aplet.


7.1 Role of a Bootloader
When power is first applied to a processor board, many elements of HW must be
initialized before even the simplest program can run.  Each architecture and 
processor has a set of predefined actions and configurations upon release of 
reset, which includes fetching initialization code from an onboard storage dev
(usually Flash memory).  This early initialization code is part of the
bootloader and is responsible for breathing life into the processor and related
HW components.

Most processors have a default address from which the first bytes of code are
fetched upon application of power and release of reset. HW designers use this
info to arrange the layout of Flash memory on the board and to select which 
address range(s) the Flash memory responds to. This way, when power is first
applied, code is fetched from a well-known and predictable address, and SW
control can be established.

The bootloader provides this early initialization code and is responsible for
initializing the board so that other programs can run.  This early 
initialization code is almost always written in the processor's native asm.

After the bootloader has performed the basic processor and platform 
initialization, its primary role is to fetch and to boot a full-blown OS. It is
responsible for locating, loading, and passing control to the primary OS. In
addition, the bootloader might have advanced features, such as the capability
to validate an OS image, upgrade itself or an OS image, or choose from among
several OS images based on a developer-defined policy. When the OS takes 
control, the bootloader is overwritten and ceases to exist.

7.4.7 U-Boot Image Format
We need to understand the image format that U-Boot requires. U-Boot expects a
small header on the image file that identifies several attributes of the image.
U-Boot provides the 'mkimage' tool (part of the U-Boot source code) to build
this image header.

Recent Linux kernel distro have built-in support for building images directly
bootable by U-Boot. Both the ARM and POWERPC branches of the kernel source tree
support a target called 'uImage'.

Browsing thru the makefile .../arch/powerpc/boot/Makefile, we see the uImage
target defining a call to an external wrapper script called 'wrapper'. The
wrapper script sets up some default variable values and eventually calls
'mkimage'.

The 'mkimage' utility creates the U-Boot header and prepends it to the supplied
kernel image.  It writes the resulting image to the final parameter passed
to 'mkimage' the value of the $ofile variable called 'uImage'

Several U-Boot commands use this header data both to verify the integrity of
the image (U-Boot also puts a CRC signature in the header) and to identify
the image type. U-Boot has a command called 'iminfo' that reads the image 
header and displays the image attributes from the target image.

7.5 Device Tree Blob (Flat Device Tree)
Different names of Device Tree Blob: flat device tree, device tree binary, or 
device tree.  

The DTB is a database that represents the HW components on a given board.  It 
has been chosen as the default mechanism to pass low-level HW info from the 
bootloader to the kernel.

Where does the DTB come from?
	+ It is provided as a courtesy by the board/architecture developers as
	  part of the Linux kernel source tree
	+ You must provide a DTB for your custom board

Device Tree Source:
the Device Tree Blob is "compiled" by a special compiler that produces the bin
in the proper form for U-Boot and Linux to understand.
It means: DTS --> Device Tree Compiler --> DTB

Device Tree Compiler:
the Device Tree Compiler (DTC) converts the human-readable device tree source
into the machine-readable binary that both U-Boot and the Linux kernel can
understand.
DTC can go in both directions: from DTS to DTB and vice versa.

Alternative Kernel Images using DTB
In many build targets available from make ARCH=<arch> help, several 
architecture-specific targets combine the DTB with the kernel image.

One good reason to do this is if you are trying to boot a newer kernel on a 
target that has an older version of U-Boot that does not suport the DTB.


8.1 Device Driver Concepts
One of the fundamental purposes of a device driver is to isolate the user
programs from ready access to critical kernel data structures and HW devices.
A well-written  device driver hides from the user the complexity and 
variability of the HW device

Loadable modules
Linux lets you add and remove kernel components at runtime.
Linux is structured as a monolithic kernel with a well-defined interface for
adding and removing device driver modules dynamically after boot time.


9.1 Linux File System Concepts
character devices: store and retrieve data in serial streams
block devices: store and retrieve data in equal-sized chunks of data at a time,
in random locations on an addressable medium.

partition: the logical division of a physical device on which a FS exists.
	When a partition is formatted with a given file system type, Linux can
	mount the corresponding FS from that partition.

inode: the fundamental data structure representing a single file.

Mounting a FS
After a FS has been created, we can mount it on a running Linux system. The 
kernel must be compiled with support for our particular FS type, either as a
compiled-in module or as a dynamically loadable module.

The following command mounts an ext2 FS on a mount point that we specify:
$ mount /dev/sdb1 /mnt/flash
This assumes that we have a directory created on our target Linux machine 
(the board) called /mnt/flash.  This is called the 'mount point' because we are
installing (mounting) the FS rooted at this point in our FS hierarchy. We are
mounting the Flash device that was assigned to the device /dev/sdb1.

mount point: any directory path on the FS that you decide, which becomes the 
top level (root) of your newly mounted device

'mount' can determine the type of file system on a properly formatted FS known
to the kernel

9.8 Network File System (NFS)
NFS enables you to export a directory on a NFS server and mount that directory
on a remote client machine as if it were a local file system.

Using NFS on your target board, en embedded developer can have access to a 
huge number of files, libraries, tools, and utilities during development and
debugging, even if the target embedded system is resource-constrained.

Your kernel must be configured with NFS support, for both the server-side
functionality and the client side.  NFS server and client functionality is
independently configured in the kernel configurations.

On your development workstation with NFS enabled, a configuration file contains
a list specifying each directory that you want to export via the NFS. That is
/etc/exports.

On an ES with NFS enabled, the following command mounts the .../workspace dir
exported by the NFS server on a mount point of our choice:
	$ mount -t nfs pluto:/home/chris/workspace /workspace
We are instructing the 'mount' command to mount a remote directory (on a 
machine named 'pluto', our development workstation) onto a local mount point
called /workspace.  For the command semantics to work, two requirements must
be met on the embedded target.
	1/ for the target to recognize the symbolic machine name pluto, it 
	   must be able to resolve the symbolic name.  To do so, just place an
	   entry in the /etc/hosts file on the target.  This allows the 
	   networking subsystem to resolve the symbolic name to its
	   corresponding IP address, e.g. an entry in the target's /etc/hosts
	   file would look like this:
	   	192.168.11.9	pluto
	2/ the embedded target must have a directory in its root directory
	   called /workspace. This is called a mount point.  The requirement
	   is that the target must have a directory created with the same name
	   as given on the 'mount' command
The mount command above causes the contents of the NFS server's 
/home/chris/workspace directory to be available on the embedded system's
/workspace path

This is useful, especially in a cross-development environment.  Each time you
make changes to the project, you need to move that application to your target
so that you can test and debug it.  Using NFS in this manner, assuming that you
are working in the NFS exported directory on you host, the changes are
immediately available on your target embedded system without the need to upload
the newly compiled project files.

Root File System on NFS
Mounting your project workspace on your target embedded system is very useful
for development and debugging because it facilitates rapid access to changes
and source code for source-level debugging.

NFS really shines as a development tool when you mount your embedded system's
root file system entirely from an NFS server.

The leading embedded Linux distros targetted at embedded systems ship tens of
thousands of files compiled and tested for the chosen target architecture.
The target file system contains just shy of a gigabyte worth of binary files
targeted at the ARM architecture.  Therefore, this would hardly fit on the 
average Flash device found on an ES.
==> This is the power of an NFS root mount.  For development purposes, it can
only increases productivity if your ES is loaded with all the tools and 
utilities you are familiar with on a Linux workstation.  Indeed, likely dozens
of command-line tools and development utilities that you have never seen can
help you shave time off your development schedule.

Configuring your ES to mount its root FS via NFS at boot time is relatively
straightforward.
	+ configure your target's kernel for NFS support. There is also a 
	  configuration option to enable root mounting of an NFS remote
	  directory. Then what remains is to somehow feed info to the kernel
	  so that it knows where to look for the NFS server. At a minimum,
	  the kernel can be passed the proper parameters on the kernel command
	  line to configure its IP port and server info on power-up.
	    console=ttyS0,115200 ip=bootp root=/dev/nfs
	  This tells the kernel to expect a root FS via NFS and to obtain the
	  relevant parameters (server name, server IP address, and root dir
	  to mount) from a BOOTP server.

	  If you are statically configuring your target's IP address, your
	  kernel command line might look like this:
	    console=ttyS0,115200	\
	    ip=192.168.11.139:192.168.11.1:255.255.255.0:coyote1:eth0:off \
	    nfsroot=192.168.11.1:/coyote-target	\
	    root=/dev/nfs
	  parameters can be found in .../Documentation/filesystem/nfsroot.txt

9.9 Pseudo File Systems
A number of file systems fall under the category of Pseudo File Systems in
the kernel configuration menu. Together they provide a range of facilities
useful in a wide range of applications.

/proc File System
the /proc FS takes its name from its original purpose: an interface that allows
the kernel to communicate info about each running process on a Linux system.

the /proc file system has become a virtual necessity for all but the simplest
of Linux systems, even embedded ones. Many user-level functions rely on the
contents of the /proc FS to do their job.

sysfs
sysfs is not representative of an actual physical device. Instead, sysfs models
specific kernel objects sush as physical devices and provides a way to associate
devices with device drivers.

sysfs provides a top-level subdirectory for several system elements, including
the system buses


10.1 MTD (Memory Technology Device) Overview
MTD is a device driver layer that provides a uniform API for intefacing with
raw Flash devices.

Contrary to popular belief, SD/MMC cards, CompactFlash cards, USB Flash drivers,
an other popular devices are not MTD devices.  MTD is unique in Linux driver
architecture. This is because MTD drivers must perform Flash-specific operations
such as the 'erase block' and 'wear leveling' operations, which have no parallel
in traditional block drivers.


12.1 Cross-Development Environment
host: the development workstation that is sitting on your desktop and running
	your favorite Linux desktop distro.
target: your embedded hardware platform.
native development: denotes the compilation and building of applicaitons on &
	for your host system
cross-development: denotes the compilation and building of applications on the
	host system that will be run on the embedded system

The host system provides the horsepower to run the compilers, debuggers,
	editors, and other utilities, and the target executes only the apps
	designed for it.

"Hello World" Embedded
a properly configured cross-development system hides a great deal of complexity
from the average application developer.

What if we want to build hello.c for a different architecture?
	When we compile an applicaiton program for a target using a 
	cross-compiler on our host machine, we must make sure that the compiler
	does not use the default host include directories or library paths.
	Using a properly configured cross-compiler is the 1st step.
	Having a well-designed cross-development environment is the 2nd.

12.2 Host System Requirements
First, you need a properly configured cross toolchain.
The next major item you need is a Linux distro targeted for your embedded
	system architecture. This includes hundreds to potentially thousands
	of files that will populate your ES' file system

In summary, your development host requires four separate and distinct 
capabilities:
	1. cross toolchain and libraries
	2. target system packages, including programs, utilities, and libraries
	3. host tools such as editors, debuggers, and utilities
	4. servers for hosting your target board

12.3 Hosting Target Boards
Having an Ethernet connection available on your target board enables the NFS
root mount configuration.

Many embedded development systems and bootloaders support TFTP and assume that
	developer will use it. TFTP is a lightweight protocol for moving files
	between a TFTP server and TFTP client over Ethernet.

Using TFTP from your bootloader to load the kernel will save you countless
	hours waiting for serial downloads

TFTP server
TFTP is a TCP/IP service that must be enabled on your workstation.  To enable
	the TFTP service, you must instruct your workstation to respond to 
	incoming TFTP packets. The easiest way to do this is to run a TFTP 
	server daemon.

Configuring this TFTP server is easy. There is a single configuration file
	called /etc/default/tftpd-hpa. This file needs to be customized to
	your particular requirements. (Listing 12-4, page 313)

BOOTP/DHCP Server
Having a DHCP server on your development host simplifies the configuration
	management for your embedded target.  When Linux boots on your target
	board, it needs to configure the Ethernet interface before the i/f
	becomes useful.  Moreover, if you are using an NFS root mount
	configuration on your target board, Linux needs to configure your
	target's Ethernet interface before the boot process can complete.

In general, Linux can use 2 methods to initialize its Ethernet/IP i/f during
boot:
	+ hard-code the Ethernet interface parameters either on the Linux 
	  kernel command line or in the default configuration, such as a static
	  IP configuration
	+ configure the kernel to automatically detect the network settings
	  at boot time ==> more flexible
	DHCP or BOOTP is the protocol your target and server use to accomplish
	the automatic detection of network settings.

A DHCP server controls the IP address assignments for IP subnets for which it
has been configured, and for DHCP or BOOTP clients that have been configured to
participate. A DHCP server listens for requests from a DHCP clients (e.g. your
target board) and assigns addresses and other pertinent info to the client as 
part of the boot process.

You must first enable the DHCP server on your Linux development workstation by:
$ /etc/init.d/dhcpd start
or
$ /etc/init.d/dhcpd restart
	dhcpd is considered a system 'service'

NFS Server
Using an NFS root mount for your target board is a very powerful development
	tool.  Here are some of the advantages of this configuration for 
	development:
	  + Your root file system is not size-restricted by your board's own
	    limited resources such as Flash memory
	  + Changes made to your application files during development are
	    immediately available to your target system
	  + You can debug and boot your kernel before developing and debugging
	    your root file system
The NFS service must be started from either your startup scripts, a graphical
	menu, or the command line, e.g.
		$ /etc/init.d/nfs start
		$ /etc/init.d/nfs restart

In addition to enabling the service, your kernel must be compiled with support
	for NFS.  Although DHCP and TFTP are both user space utilities, NFS
	requires kernel support.  This is true on both our development
	workstation and your target board.  Figure 12-2 shows the configuration
	options for NFS in the kernel.  Notice that there are config options
	for both NFS server and client support.  Note also the option
	"Root File System on NFS".  Your target kernel must have this option
	configured for NFS root mount operation

The NFS server gets its instructions from an export file located on your
	development workstation.  It is commonly found in /etc/exports.
	Entries in this file allow a client to remotely mount any of those.

You can test your NFS configuration right from your workstation.  Assuming that
	you have NFS services enabled (which requires that both the NFS server 
	and client components are enabled), you can mount a local NFS export
	as you would mount any other file system:
	$ mount -t nfs localhost:/home/chris/workspace /mnt/remote
If this command succeeds and the files in .../workspace are available on 
	/mnt/remote, your NFS server configuration is working

Target NFS Root mount
A set of details must be correct before it will work.
	1. configure your NFS server, and export a proper target FS for your
	   architecture
	2. configure your target kernel with NFS client services and root FS
	   on NFS
	3. enable kernel-level autoconfiguration of your target's Ethernet i/f
	4. provide your target Ethernet IP configuration using the kernel cmd
	   line or static kernel configuration option
	5. provide a kernel command line enabled for NFS


